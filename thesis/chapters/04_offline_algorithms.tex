% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Offline Algorithms}\label{chapter:offline_algorithms}

To later measure the performance of online algorithms, we must first describe efficient offline algorithms that can be used to find optimal (or nearly optimal) solutions. In this chapter, we therefore describe algorithms for fractional and integral smoothed convex optimization. We begin with a general argument on the fractional case. Then, we turn to specific algorithms described in the literature for the uni-dimensional and multi-dimensional settings.

In our general argument we want to use that the cost function $c_{\text{SCO}}$ is convex on $\mathcal{X}^T$.

\begin{lemma}
$c_{\text{SCO}}(X) = \sum_{t=1}^T f_t(X_t) + \norm{X_t - X_{t-1}}$ is convex on $\mathcal{X}^T$.
\end{lemma}
\begin{proof}
By definition $f_t$ is convex on $\mathcal{X}$. Every norm $\norm{\cdot}$ on $\mathcal{X}$ is convex by the triangle inequality as shown by the following argument: \begin{align*}
    \forall x, y \in \mathcal{X}.\ \forall \lambda \in [0,1].\ \norm{\lambda x + (1 - \lambda) y} \leq \norm{\lambda x} + \norm{(1 - \lambda) y} = \lambda \norm{x} + (1 - \lambda) \norm{y}.
\end{align*} In total we get that the sum of convex functions is also convex.
\end{proof}

As the offline variant of SCO is simply the problem of minimizing $c_{\text{SCO}}$ it is easy to see that we can use a convex optimization solver to obtain the optimal schedule $\hat{X}$. Throughout our discussion of algorithms, we denote schedules by $X$ and optimal solutions by $\hat{\cdot}$. Further, as we do not impose any constraints beyond the bounds of the decision space $\mathcal{X} \subset \mathbb{R}^d$ it suffices to find the local optimum of $c_{\text{SCO}}$ with respect to $\mathcal{X}$. By the convexity of $c_{\text{SCO}}$ we know that any local optimum will also be a global optimum \cite{Bubeck2015}. We are able to choose the algorithm for finding the local optimum based on our knowledge of the properties of $c_{\text{SCO}}$. If, for example, $c_{\text{SCO}}$ is differentiable on $\mathcal{X}$ we could use gradient descent. In general, even if we cannot be sure of the differentiability of $c_{\text{SCO}}$ we can always find a subgradient in the interior of $\mathcal{X}$ \cite{Bubeck2015}. We denote by $\partial c_{\text{SCO}}(x)$ the set of subgradients of $c_{SCO}$ at the point $x \in \mathcal{X}$.

We are interested in finding good approximations of $\hat{x}$. To that end, we call a solution $\hat{x}$ \emph{$\epsilon$-optimal}\index{$\epsilon$-optimal solution} if $\norm{g(\hat{x})}^2 \leq \epsilon$ where $g \in \partial c_{\text{SCO}}(\hat{x})$. Using the projected subgradient method we are able to find an $\epsilon$-optimal solution $\hat{x}$ in $\mathcal{O}(1 / \epsilon^2)$ iterations \cite{Boyd2003}. Note that this convergence rate is dimension-independent.

We have seen a general method of solving high-dimensional (fractional) SCO, though the convergence rate could be increased by using an accelerated gradient method in case the hitting costs are strongly convex or conditional gradient descent in case the hitting costs are smooth \cite{Bubeck2015}. Denoting the complexity of computing the hitting cost $f_t$ by $\mathcal{O}(C)$ and the convergence rate by $\mathcal{O}(O_{\epsilon})$ we obtain a total complexity of $\mathcal{O}(T C O_{\epsilon})$. We observe, however, that our method does not extend to the integral case which is of particular importance for the application of right-sizing data centers. We therefore devote much of the remaining chapter to the discussion of algorithms for integral variants of SCO.

\section{Uni-Dimensional}

We now limit our attention to the uni-dimensional setting, i.e. $\mathcal{X} \subset \mathbb{R}$.

\subsection{Backward-Recurrent Capacity Provisioning}

Before we begin discussing algorithms for Int-SSCO, we discuss a simple backward-recurrent algorithm for SSCO proposed by \citeauthor*{Lin2011} based on the idea of capacity provisioning \cite{Lin2011}. They extend this idea to formulate an online algorithm which we discuss in \autoref{section:online_algorithms:ud:lazy_capacity_provisioning}.

\citeauthor*{Lin2011} observed that the optimal offline solution can be characterized by two bounds which correspond to charging the switching cost for powering-up and powering-down servers, respectively. Let $\tau \in [T]$ be a time slot. Then the optimal offline solution $\hat{X}_{\tau}$ during time slot $\tau$ is lower bounded by $X_{\tau}^L$ where $X^L$ is the smallest vector minimizing \begin{align*}
    c_{\tau}^L(X) = \sum_{t=1}^{\tau} f_t(X_t) + \beta (X_t - X_{t-1})^+.
\end{align*} Conversely, $\hat{X}_{\tau}$ is upper bounded by $X_{\tau}^U$ where $X^U$ is the largest vector minimizing \begin{align*}
    c_{\tau}^L(X) = \sum_{t=1}^{\tau} f_t(X_t) + \beta (X_{t-1} - X_t)^+.
\end{align*} Overall we have $X_{\tau}^L \leq \hat{X}_{\tau} \leq X_{\tau}^U$ \cite{Lin2011}. Note that in the case of the lower bound the switching cost is paid for powering-up a server while in the case of the upper bound the switching cost is paid for powering-down a server. Further, it is easy to see that the bounds for time slot $\tau$ do not depend on any time slots $t > \tau$.

An optimal offline algorithm can therefore be described as determining the optimal schedule moving backwards in time. We begin by setting $\hat{X}_{T+1} = 0$. For each previous time slot $\tau$, we set $\hat{X}_{\tau} = \hat{X}_{\tau + 1}$ unless this violates the bounds, in which case we make the smallest possible change: \begin{align*}
    \hat{X}_{\tau} = \begin{cases}
        0 & \tau > T \\
        (\hat{X}_{\tau+1})_{X_{\tau}^L}^{X_{\tau}^U} & \tau \leq T
    \end{cases}
\end{align*} where $(\hat{X}_{\tau+1})_{X_{\tau}^L}^{X_{\tau}^U}$ is the projection of $\hat{X}_{\tau+1}$ onto $[X_{\tau}^L, X_{\tau}^U]$ \cite{Lin2011}. The resulting algorithm is described in \autoref{alg:brcp}. We are able to use algorithms for convex optimization to compute the upper and lower bounds. Hence, we obtain a complexity of $\mathcal{O}(T^2 C O_{\epsilon})$ for $\epsilon$-optimal upper and lower bounds.

\begin{algorithm}
    \caption{Backward-Recurrent Capacity Provisioning \cite{Lin2011}}\label{alg:brcp}
    \SetKwInOut{Input}{Input}
    \Input{$\mathcal{I}_{\text{SSCO}} = (T \in \mathbb{N}, m \in \mathbb{N}, \beta \in \mathbb{R}_{>0}, (f_1, \dots, f_T) \in (\mathbb{R}_{\geq 0} \to \mathbb{R}_{\geq 0})^T)$}
    $\hat{X}_{T+1} \gets 0$\;
    \For{$\tau \gets T$ \KwTo $1$}{
        $\hat{X}_{\tau} \gets (\hat{X}_{\tau+1})_{X_{\tau}^L}^{X_{\tau}^U}$\;
    }
    \Return $\hat{X}$\;
\end{algorithm}

\subsection{Graph-Based Optimal Integral Algorithm}

We now turn to the uni-dimensional integral case of simplified smoothed convex optimization, i.e. our decision space is given as $\mathcal{X} := [m]_0$ where $m \in \mathbb{N}$ is the maximum number of servers a data center can employ at the same time. As our decision space is discrete and finite, it is natural to model our problem as a weighted directed graph which is comprised of vertices $v_{t,j}$ for each $t \in [T]$ and $j \in [m]_0$ describing the state that during time slot $t$ exactly $j$ servers are active. For the initial and final state $X_0 = X_{T+1} = 0$ we have two additional vertices $v_{0,0}$ and $v_{T+1,0}$. For each vertice associated with time slot $t \in [T]_0$ we add an edge to all vertices associated with the subsequent time slot $t + 1$. With each edge from $v_{t,i}$ to $v_{t+1,j}$ we associate the switching cost incurred by the represented action, i.e. $\beta (j - i)^+$, and the hitting cost incurred by the state represented by the vertice $v_{t+1,j}$, i.e. $f_{t+1}(j)$. The structure of this graph is presented in [TODO: add figure].

\citeauthor*{Albers2018} show that an optimal schedule is given by the shortest path from $v_{0,0}$ to $v_{T+1,0}$ within our constructed graph \cite{Albers2018}. We observe that our graph follows a special structure. Each vertice representing an action during time slot $t$ can only be reached by a path that includes precisely one vertice representing time slots $0$ through $t - 1$ and, crucially, whether this path is optimal up to time $t$ does not depend on any time slot past $t$. Bellman's principle of optimality states that any part of an optimal path must itself be optimal \cite{Bellman1954}. Based on this principle and using our previous observation we are able to use dynamic programming to sequentially determine the optimal paths to the vertices of time slot $t$.

Moreover, we observe a second property of the graph of interest. Namely, for each time slot $t \in [T]$ (in the following called columns) the graph has exactly $m$ vertices (in the following called rows) which can only be reached through edges from vertices representing time slot $t - 1$ and all outgoing edges lead to vertices representing time slot $t + 1$. We are therefore able to iteratively solve the problem of finding a shortest path for subgraphs of our original graph using binary search. During each iteration we only consider a constant number of rows.

To simplify the selection of rows, the algorithms assumes $m$ to be a power of two. Instances where $m$ is not a power of two can be transformed by choosing $m' = 2^{\lceil \log_2 m \rceil}$ and \begin{align*}
    f'_t(x) = \begin{cases}
        f_t(x) & x \leq m \\
        x (f_t(m) + \epsilon) & \text{otherwise} \\
    \end{cases}
\end{align*} for $\epsilon > 0$. The algorithm using $\log_2 m - 1$ iterations is described in \autoref{alg:ud:optimal_graph_search}. During each iteration the algorithm finds the shortest path in a subgraph comprised of only five rows in $\mathcal{O}(T C)$ time. Overall, we thus find an optimal schedule in $\mathcal{O}(T C \log_2 m)$ time.

\begin{algorithm}
    \caption{Uni-Dimensional Optimal Graph Search \cite{Albers2018}}\label{alg:ud:optimal_graph_search}
    \KwIn{$\mathcal{I}_{\text{Int-SSCO}} = (T \in \mathbb{N}, m \in \mathbb{N}, \beta \in \mathbb{R}_{>0}, (f_1, \dots, f_T) \in ([m]_0 \to \mathbb{R}_{\geq 0})^T)$ with $m$ a power of two}
    \eIf{$m > 2$}{$K \gets \log_2 m - 2$ }{$K \gets 0$ }
    $V^K \gets \{v_{0,0}, v_{T+1,0}\} \cup \{v_{t,\xi m / 4} \mid t \in [T], \xi \in [4]_0\}$\;
    $\hat{X}^K \gets \text{\ref{proc:ud:optimal_graph_search:shortest_path}}(\mathcal{I}_{Int-SSCO}, V^K)$\;
    \If{$K > 0$}{\For{$k \gets K - 1$ \KwTo $0$}{
        $V_t^k \gets \{\hat{X}_t^{k+1} + \xi 2^k \mid \xi \in \{-2, -1, 0, 1, 2\}\} \cap [m]_0$\;
        $V^k \gets \{v_{0,0}, v_{T+1,0}\} \cup \{v_{t,j} \mid t \in [T], j \in V_t^k\}$\;
        $\hat{X}^k \gets \text{\ref{proc:ud:optimal_graph_search:shortest_path}}(\mathcal{I}_{Int-SSCO}, V^k)$\;
    }}
    \Return $\hat{X}^0$\;
\end{algorithm}

\begin{function}
	\caption{ShortestPath($\mathcal{I}, V$)}\label{proc:ud:optimal_graph_search:shortest_path}
	\For{$t \gets 1$ \KwTo $T$}{
        \ForEach{$v_{t,j} \in V$}{
            $\hat{c}^{v_{t,j}} \gets \infty$\;
            \ForEach{$v_{t-1,i} \in V$}{
                $c^{v_{t,j}} \gets \hat{c}^{v_{t-1,i}} + f_t(j) + \beta (j - i)^+$\;
                \If{$c^{v_{t,j}} < \hat{c}^{v_{t,j}}$}{
                    $\hat{c}^{v_{t,j}} \gets c^{v_{t,j}}$\;
                    $\hat{X}^{v_{t,j}} \gets \hat{X}^{v_{t-1,i}}$\;
                }
            }
            $\hat{X}_t^{v_{t,j}} \gets j$\;
        }
    }
    $\hat{v} \gets \argmin_{v_{T,j} \in V} \hat{c}^{v_{T,j}}$\;
	\Return $\hat{X}^{\hat{v}}$\;
\end{function}

\section{Multi-Dimensional}

\subsection{Graph-Based Optimal Integral Algorithm}

We now lift the restriction on $d$ and also consider multi-dimensional instances of Int-SSCO. Again, an intuitive approach is to model the offline problem using a graph. Previously, with $d = 1$, the vertices where arranged in a two-dimensional grid (time being the first dimension). We now arrange the vertices in a $(d+1)$-dimensional grid. We call $x = (x_1, \dots, x_d) \in \mathcal{M}$ a \emph{configuration}\index{configuration} where $M_k := [m_k]_0$ and $\mathcal{M} := M_1 \times \dots \times M_d = \mathcal{X}$ is the set of all configurations. For each configuration $x$ and each time slot $t$ we introduce two vertices. $v_{t,x}^{\uparrow}$ represents the configuration in the beginning of time slot $t$ while $v_{t,x}^{\downarrow}$ represents the configuration at the end of time slot $t$. Thus, the first dimension has $2 T$ \emph{layers} where each layer only consists of powering-up or powering-down vertices.

In our graph we have edges $e_{t,x,k}^{\uparrow}$ representing the powering-up of a server of type $k$ in the beginning of time slot $t$, edges $e_{t,x}^{\text{op}}$ representing operating configuration $x$ during time slot $t$, edges $e_{t,x,k}^{\downarrow}$ representing the powering-down of a server of type $k$ at the end of time slot $t$, and edges $e_{t,x}^{\rightarrow}$ transitioning to the next time slot. For each $k \in [d]$ and $x = (x_1, \dots, x_d) \in [m_1]_0 \times \dots \times [m_k - 1]_0 \times \dots \times [m_d]_0$ let $x' = (x_1, \dots, x_k + 1, \dots, x_d)$. We add an edge $e_{t,x,k}^{\uparrow}$ between $v_{t,x}^{\uparrow}$ and $v_{t,x'}^{\uparrow}$ with weight $\beta_k$ and another edge $e_{t,x,k}^{\downarrow}$ between $v_{t,x'}^{\downarrow}$ and $v_{t,x}^{\downarrow}$ with weight $0$. For each time slot $t \in [T]$ and $x \in \mathcal{M}$, we add the edge $e_{t,x}^{\text{op}}$ from $v_{t,x}^{\uparrow}$ to $v_{t,x}^{\downarrow}$ with weight $f_t(x)$. Lastly, for each $t \in [T-1]$ and $x \in \mathcal{M}$, we add the edge $e_{t,x}^{\rightarrow}$ from $v_{t,x}^{\downarrow}$ to $v_{t+1,x}^{\uparrow}$ with weight $0$.  To simplify the algorithm we add an additional vertice $v_{T+1,\mathbf{0}}^{\uparrow}$ which can be reached through the edge $e_{t,\mathbf{0}}^{\rightarrow}$ from $v_{t,\mathbf{0}}^{\downarrow}$. The structure of this graph is presented in [TODO: add figure].

Any path from $v_{0,\mathbf{0}}^{\uparrow}$ to $v_{T,\mathbf{0}}^{\downarrow}$ must traverse exactly one edge $e_{t,x}^{\text{op}}$ for each time slot $t \in [T]$. The \emph{induced schedule} $X^P$ by a path $P$ assigns each time slot $t$ the configuration $x$ of the traversed edge $e_{t,x}^{\text{op}}$. \citeauthor*{Albers2021_2} show that a shortest path from $v_{0,\mathbf{0}}^{\uparrow}$ to $v_{T,\mathbf{0}}^{\downarrow}$ induces an optimal schedule \cite{Albers2021_2}. They also show that the cost of any induced schedule $X^P$ is given by the cost of the path $P$ when the sub-path between $v_{t,X_t^P}^{\downarrow}$ and $v_{t+1,X_{t+1}^P}^{\uparrow}$ is the shortest sub-path for all $t \in [T-1]$.

Again, we are able to use dynamic programming to obtain a shortest path from $v_{0,\mathbf{0}}^{\uparrow}$ to $v_{T,\mathbf{0}}^{\downarrow}$. Using Bellman's principle of optimality we conclude that any such shortest path $P$ must consist of shortest sub-paths between the vertices of two subsequent layers. The algorithms works as follows: For each layer of each time slot $t$, we sequentially update the shortest path to each vertice of that layer. In layers consisting of powering-up vertices, we begin with the vertice $v_{t,\mathbf{0}}^{\uparrow}$ and then sequentially increase the values of each dimension beginning with dimension $1$. It is easy to see that by updating the vertices in this order, the predecessors of any newly reached vertice have already been updated. Conversely, in layers consisting of powering-down vertices, we begin with the vertice $v_{t,(m_1,\dots,m_d)}^{\downarrow}$ and iterate the dimensions from dimension $d$ through dimension $1$. The resulting algorithm is described in \autoref{alg:md:optimal_graph_search}. We denote by $\hat{X}^v$ the optimal schedule up to vertice $v$ and by $\hat{c}^v$ the cost of the optimal schedule up to vertice $v$.

\begin{algorithm}
    \caption{Multi-Dimensional Optimal Graph Search \cite{Albers2021_2}}\label{alg:md:optimal_graph_search}
    \KwIn{$\mathcal{I}_{\text{Int-SSCO}} = (T \in \mathbb{N}, m \in \mathbb{N}^d, \beta \in \mathbb{R}_{>0}^d, (f_1, \dots, f_T) \in (\mathcal{M} \to \mathbb{R}_{\geq 0})^T)$}
    \For{$t \gets 1$ \KwTo $T$}{
        $(\hat{X}, \hat{c}) \gets \text{\ref{proc:md:optimal_graph_search:handle_first_layer}}(\mathcal{I}_{Int-SSCO}, \mathcal{M}, \hat{X}, \hat{c}, t, 1, \{\mathbf{0}\})$\;
        $(\hat{X}, \hat{c}) \gets \text{\ref{proc:md:optimal_graph_search:handle_second_layer}}(\mathcal{I}_{Int-SSCO}, \mathcal{M}, \hat{X}, \hat{c}, t, d, \{(m_1,\dots,m_d)\})$\;
    }
    \Return $\hat{X}^{v_{T,\mathbf{0}}^{\downarrow}}$\;
\end{algorithm}

Here, the functions \ref{proc:md:optimal_graph_search:handle_first_layer} and \ref{proc:md:optimal_graph_search:handle_second_layer} update the vertices of the respective layer during time slot $t$. $k$ denotes the dimension that is expanded in the current iteration and $\mathcal{B}$ is the set of configurations from previous iterations the current expansion is based upon. $E \in V \times \mathbb{R}$ is the set of all predecessors of the vertice $v_{t,x}^{\xi}$ along with the cost of the respective edge.

\begin{function}
	\caption{HandleFirstLayer($\mathcal{I}, \mathcal{M}, \hat{X}, \hat{c}, t, k, \mathcal{B}$)}\label{proc:md:optimal_graph_search:handle_first_layer}
	\lIf{$k > d$}{
	    \Return $(\hat{X}, \hat{c})$
	}
	$\mathcal{B}' \gets \mathcal{B}$\;
	\ForEach{$y \in \mathcal{B}$}{
	    \ForEach{$j \in M_k$}{
	        $x \gets y_{k \gets j}$\;
	        $E \gets \{(v_{t,x_{l \gets P_k(x_l)}}^{\uparrow}, \beta_l (x_l - P_k(x_l))) \mid l \in [k]_0, x_l > 0\}$\;
	        \If{$t > 1$}{$E \gets \{(v_{t-1,x}^{\downarrow}, 0)\} \cup E$\;}
	        $(\hat{X}, \hat{c}) \gets \text{\ref{proc:md:optimal_graph_search:update_paths}}(\hat{X}, \hat{c}, v_{t,x}^{\uparrow}, E)$\;
	        $\mathcal{B}' \gets \mathcal{B}' \cup \{x\}$\;
        }
    }
    \Return $\text{\ref{proc:md:optimal_graph_search:handle_first_layer}}(\mathcal{I}_{Int-SSCO}, \mathcal{M}, \hat{X}, \hat{c}, t, k+1, \mathcal{B}')$\;
\end{function}

\begin{function}
	\caption{HandleSecondLayer($\mathcal{I}, \mathcal{M}, \hat{X}, \hat{c}, t, k, \mathcal{B}$)}\label{proc:md:optimal_graph_search:handle_second_layer}
	\lIf{$k < 1$}{
	    \Return $(\hat{X}, \hat{c})$
	}
	$\mathcal{B}' \gets \mathcal{B}$\;
	\ForEach{$y \in \mathcal{B}$}{
	    \ForEach{$j \in M_k$}{
	        $x \gets y_{k \gets j}$\;
	        $E \gets \{(v_{t,x}^{\uparrow}, f_t(x))\} \cup \{(v_{t,x_{l \gets N_k(x_l)}}^{\downarrow}, 0) \mid l \in [k]_0, x_l < m_l\}$\;
	        $(\hat{X}, \hat{c}) \gets \text{\ref{proc:md:optimal_graph_search:update_paths}}(\hat{X}, \hat{c}, v_{t,x}^{\downarrow}, E)$\;
	        $\mathcal{B}' \gets \mathcal{B}' \cup \{x\}$\;
        }
    }
    \Return $\text{\ref{proc:md:optimal_graph_search:handle_second_layer}}(\mathcal{I}_{Int-SSCO}, \mathcal{M}, \hat{X}, \hat{c}, t, k-1, \mathcal{B}')$\;
\end{function}

$x_{k \gets j}$ denotes the update of configuration $x$ in dimension $k$ to the value $j$. We assume $M_k$ to be in ascending order for the first layer and in descending order for the second layer. $P_k(j)$ and $N_k(j)$ denote the previous and next values to $j$ in $M_k$, respectively. We keep the definitions abstract to allow for the generalization of this algorithm to an approximation algorithm. In the case of the optimal algorithm $P_k(j) = j-1$ and $N_k(j) = j+1$ for all $k \in [d]$. It is easy to verify that during the last iteration $\mathcal{B} = \mathcal{M}$.

\begin{function}
	\caption{UpdatePaths($\hat{X}, \hat{c}, v_{t,x}^{\xi}, E$)}\label{proc:md:optimal_graph_search:update_paths}
	$\hat{c}^{v_{t,x}^{\xi}} \gets \infty$\;
	\ForEach{$(v_{\tau,y}^{\kappa}, c^{v_{\tau,y}^{\kappa}}) \in E$}{
	    $c^{v_{t,x}^{\xi}} \gets \hat{c}^{v_{\tau,y}^{\kappa}} + c^{v_{\tau,y}^{\kappa}}$\;
	    \If{$c^{v_{t,x}^{\xi}} < \hat{c}^{v_{t,x}^{\xi}}$}{
	        $\hat{c}^{v_{t,x}^{\xi}} \gets c^{v_{t,x}^{\xi}}$\;
	        $\hat{X}^{v_{t,x}^{\xi}} \gets \hat{X}^{v_{\tau,y}^{\kappa}}$\;
	        \lIf{$\kappa = {\uparrow} \land \xi = {\downarrow}$}{$\hat{X}_t^{v_{t,x}^{\xi}} \gets x$}
	    }
	}
    \Return $(\hat{X}, \hat{c})$\;
\end{function}

The \autoref{proc:md:optimal_graph_search:update_paths} determines the shortest path to $v$ through its predecessors $E$ and updates the optimal schedule $\hat{X}^v$ (if necessary) and optimal cost $\hat{c}^v$.

\ref{proc:md:optimal_graph_search:update_paths} runs in $\mathcal{O}(|E|)$ time. Assuming $d$ is a constant, $|E| \in \mathcal{O}(1)$ and thus \ref{proc:md:optimal_graph_search:handle_first_layer} runs in $\mathcal{O}(|\mathcal{M}|)$ time while \ref{proc:md:optimal_graph_search:handle_second_layer} runs in $\mathcal{O}(|\mathcal{M}| C)$ time. Therefore, the overall time complexity of \ref{alg:md:optimal_graph_search} is in $\mathcal{O}(T |\mathcal{M}| C)$ where $\mathcal{O}(T |\mathcal{M}|)$ is the size of the underlying graph. Note, that this running time is not polynomial in the size of the problem instance which is given by $\mathcal{O}(T + \sum_{k=1}^d \log_2 m_k)$ as $|\mathcal{M}| \in \mathcal{O}(\prod_{k=1}^d m_k)$. For this reason, \citeauthor*{Albers2021_2} developed an approximation algorithm which we discuss next.

\subsection{Graph-Based Polynomial-Time Integral Approximation Algorithm}

\citeauthor*{Albers2021_2} extend the graph-based algorithm from the previous section to a polynomial-time approximation scheme \cite{Albers2021_2}. Their algorithm is a generalization of an approximation algorithm that was previously proposed by \citeauthor*{Kappelmann2017} \cite{Kappelmann2017}. The idea is to restrict the possible values of $x_{t,k}$. In addition to requiring $x_{t,k} \in [m_k]_0$, we also require $x_{t,k}$ to be a power of some $\gamma > 1$. The possible numbers of active servers of type $k$ is now given as \begin{align*}
    M_k^{\gamma} := &\{0, m_k\} \cup \{\lfloor\gamma^i\rfloor \in [m_k]_0 \mid i \in \mathbb{N}\} \cup \{\lceil\gamma^i\rceil \in [m_k]_0 \mid i \in \mathbb{N}\} \\
                    &\{0, \lfloor\gamma^1\rfloor, \lceil\gamma^1\rceil, \lfloor\gamma^2\rfloor, \lceil\gamma^2\rceil, \dots, m_k\}.
\end{align*} It is easy to see that the ratio between two consecutive values in the ordered set $M_k^{\gamma}$ is not larger than $\gamma$. Also, $|M_k^{\gamma}| \in \mathcal{O}(\log_{\gamma} m_k)$. We define $\mathcal{M}^{\gamma} := M_1^{\gamma} \times \dots \times M_d^{\gamma}$ which results in $|\mathcal{M}^{\gamma}| \in \mathcal{O}(\prod_{k=1}^d \log_{\gamma} m_k)$.

Let $G^{\gamma}$ be the described graph parametrized by $\gamma > 1$. \citeauthor*{Albers2021_2} show that given a shortest path $P^{\gamma}$ in $G^{\gamma}$, its induced schedule $X^{P^{\gamma}}$ is a $(2\gamma + 1)$-approximation of the optimal schedule $\hat{x}$ \cite{Albers2021_2}. Further, the total number of vertices in $G^{\gamma}$ is \begin{align*}
    \mathcal{O}(T |\mathcal{M}|^{\gamma}) = \mathcal{O}(T \prod_{k=1}^d \log_{1+\epsilon} m_k)
\end{align*} as $|M_k^{\gamma}| \in \mathcal{O}(\log_{\gamma} m_k) = \mathcal{O}(\log_{1 + \epsilon} m_k)$. Using the same graph search algorithm that was described in the previous section we are able to find $X^{P^{\gamma}}$ in $\mathcal{O}(T C \prod_{k=1}^d \log_{1 + \epsilon} m_k)$ time. Hence, setting $\gamma = 1 + \epsilon / 2$ for some $\epsilon > 0$ yields a $(1+\epsilon)$-approximation.

One important question regarding the approximation algorithm remains. Namely, how to efficiently compute the powers of gamma $\gamma^i$ where $\gamma^i \in [m_k]$ for some $k \in [d]$. There are two obvious approaches. First, to iteratively increase $i \in \mathbb{N}$ until $\gamma^i$ is greater than $m_k$. We then keep track of all $\lfloor\gamma^i\rfloor$ and $\lceil\gamma^i\rceil$ that were generated in this way. Another approach is to iterate over all $j \in [m_k]_0$ and determine whether they are used by the approximation. It is easy to see that $j$ is used by the approximation if and only if $\lfloor\log_{\gamma} (j-1)\rfloor \neq \lfloor\log_{\gamma} (j+1)\rfloor$ or $\lceil\log_{\gamma} (j-1)\rceil \neq \lceil\log_{\gamma} (j+1)\rceil$. We give the formal proof with the following lemma.

\begin{lemma}
Let $\gamma > 1$, $k \in [d]$, and $j \in [m_k]$. Then there exists an $i \in \mathbb{N}$ such that $\lfloor\gamma^i\rfloor = j$ or $\lceil\gamma^i\rceil = j$ if and only if $\lfloor\log_{\gamma} (j-1)\rfloor \neq \lfloor\log_{\gamma} (j+1)\rfloor$ or $\lceil\log_{\gamma} (j-1)\rceil \neq \lceil\log_{\gamma} (j+1)\rceil$.
\end{lemma}
\begin{proof}
First, we suppose there exists such an $i$. If $\lfloor\gamma^i\rfloor = j$ then $\lfloor\gamma^i\rfloor > j-1$ and $\gamma^i > j-1 \iff i > \log_{\gamma} (j-1)$, but $\log_{\gamma} (j+1) \geq i$ which implies $\lfloor\log_{\gamma} (j-1)\rfloor \neq \lfloor\log_{\gamma} (j+1)\rfloor$. If, on the other hand, $\lceil\gamma^i\rceil = j$ then $\lceil\gamma^i\rceil < j+1$ and $\gamma^i < j+1 \iff i < \log_{\gamma} (j+1)$, but $\log_{\gamma} (j-1) \leq i$ which implies $\lceil\log_{\gamma} (j-1)\rceil \neq \lceil\log_{\gamma} (j+1)\rceil$.

Now, we assume such an $i$ does not exist. In other words, for all $i \in \mathbb{N}$ we have $\lceil\gamma^i\rceil < j < \lfloor\gamma^{i+1}\rfloor$. In particular, we observe that the closest exponents to $\gamma$ resulting in a natural number are $\lfloor\log_{\gamma} j \rfloor =: i$ and $\lceil\log_{\gamma} j \rceil =: i+1$. Hence, \begin{align}
    \lceil\gamma^i \rceil       &\leq j-1 \label{eq:approx_alg:exp1} \\
    \lfloor\gamma^{i+1} \rfloor &\geq j+1. \label{eq:approx_alg:exp2}
\end{align} From \autoref{eq:approx_alg:exp1} we follow $\lfloor\log_{\gamma} j-1 \rfloor = \lfloor\log_{\gamma} j \rfloor$ and from \autoref{eq:approx_alg:exp2} we follow $\lfloor\log_{\gamma} j+1 \rfloor = \lfloor\log_{\gamma} j \rfloor$. We thus established $\lfloor\log_{\gamma} (j-1)\rfloor = \lfloor\log_{\gamma} (j+1)\rfloor$. The other statement $\lceil\log_{\gamma} (j-1)\rceil = \lceil\log_{\gamma} (j+1)\rceil$ follows analogously.
\end{proof}

We observe that the first approach takes $\mathcal{O}(\log_{\gamma} m_k)$ time while the second approach takes $\mathcal{O}(m_k)$ time. Hence, the first approach is faster than the second approach if $\log_{\gamma} m_k < m_k$, or equivalently $m_k < \gamma^{m_k}$. We have discussed a polynomial time approximation scheme for multi-dimensional instances of Int-SSCO.
