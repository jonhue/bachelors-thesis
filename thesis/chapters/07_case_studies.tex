% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Case Studies}\label{chapter:case_studies}

This chapter examines the performance of the previously described models and algorithms using real server traces. Thereby we focus on two aspects: First, we are interested in how well the discussed algorithms compare in absolute terms and relative to each other. Second, we are interested in the general promise of dynamically right-sizing data centers, which we study by conservatively estimating cost savings and relating them to previous research.

\section{Method}

First, we describe our experimental setup. We begin with a detailed discussion of the characteristics of the server traces, which we use as a basis for our analysis. Then, we examine the underlying assumptions of our analysis. This is followed by a discussion of alternative approaches to right-sizing data centers, which we use as a foundation for estimating the cost savings resulting from dynamic right-sizing of data centers. Next, we describe the general model parameters we use in our analysis and relate them to previous research. Lastly, we introduce the precise performance metrics used in the subsequent sections.

Throughout our experiments, we seek to determine conservative approximations for the resulting performance and cost savings. Our experimental results were obtained on a machine with 16 GB memory and an Intel Core i7-8550U CPU with a base clock rate of 1.80GHz.

\subsection{Traces}\label{section:case_studies:method:traces}

We use several traces with varying characteristics for our experiments. Some traces are from clusters rather than individual data centers. However, to simplify our analysis, we assume traces apply to a single data center without restricting the considered server architectures.

\citeauthor*{Amvrosiadis2018}~\cite{Amvrosiadis2018} showed that the characteristics of traces vary drastically even within a single trace when different subsets are considered individually. Their observation shows that it is crucial to examine long server traces and various server traces from different sources to gain a proper perspective of real-world performance.

\subsubsection{Characteristics}

To understand the varying effectiveness of dynamic right-sizing for the considered traces, we first analyze the properties of the given traces.

The most immediate and fundamental properties of a trace are its duration, the number of appearing jobs, the number of job types, and the underlying server infrastructure -- especially whether this infrastructure is homogeneous or heterogeneous.

Then, we also consider several more specific characteristics. The \emph{interarrival time}\index{interarrival time} (or submission rate) of jobs is the distribution of times between job arrivals. This distribution indicates the average system load as well as load uniformity. The \emph{peak-to-mean ratio (PMR)}\index{peak-to-mean ratio} is defined as the ratio of the maximum load and the mean load. It is a good indicator of the uniformity of loads. We refer to time slots as \emph{peaks}\index{peak load} when their load is greater than the 0.9-quantile of loads. We call the ratio of the 0.9-quantile of loads and the mean load \emph{true peak-to-mean-ratio (TPMR)}\index{true peak-to-mean ratio} as it is less sensitive to outliers than the PMR. We refer to periods between peaks as \emph{valleys}\index{valley}. More concretely, we refer to the time distance between two consecutive peaks as \emph{peak distance}\index{peak distance} and the number of consecutive time slots up to a time slot with a smaller load as \emph{valley length}\index{valley length}. Further, we say that a trace follows a \emph{diurnal pattern}\index{diurnal pattern} if during every 24 hours, excluding the final day, there is at least one valley spanning 12 hours or more. We exclude the final day as the final valley might be shortened by the end of the trace.

We also consider some additional information included in some traces, such as the measured scheduling rate (or queuing delay), an indicator for utilization.
% and, if provided, the distribution of the measured utilization of servers which may be an indicator for resource over-commitment.

\subsubsection{Overview}

We now give an overview of all used traces. For our initial analysis, we use a time slot length of 10 minutes.

\paragraph{MapReduce\footnote{MapReduce is a programming model for processing and generating large data sets in a functional style~\cite{Dean2004}} Workload from a Hadoop\footnote{Apache Hadoop is an open-source software for managing clusters} Cluster at Facebook~\cite{SWIM2013}} This trace encompasses three day-long traces from 2009 and 2010, extracted from a 6-month and a 1.5-month-long trace containing 1 million homogeneous jobs each. The traces are visualized in \cref{fig:facebook:histogram} and summarized in \cref{tab:facebook}. The cluster consists of 600 machines which we assume to be homogeneous. For the trace from 2010, we adjust the maximum number of servers to 1000 as otherwise the trace is infeasible under our models. \Cref{fig:facebook:schedule} visualizes the corresponding dynamic and static offline optimal schedules under our second model (which is described in \cref{section:case_studies:traces:model-parameters}). The trace was published by \citeauthor*{SWIM2013}~\cite{SWIM2013} as part of the SWIM project at UC Berkeley.

\begin{figure}
    \begin{subfigure}[b]{.3425\linewidth}
    \resizebox{\textwidth}{!}{\input{thesis/figures/facebook_2009_0_histogram.tex}}
    \caption{2009-0}
    \end{subfigure}
    \begin{subfigure}[b]{.32\linewidth}
    \resizebox{\textwidth}{!}{\input{thesis/figures/facebook_2009_1_histogram.tex}}
    \caption{2009-1}
    \end{subfigure}
    \begin{subfigure}[b]{.32\linewidth}
    \resizebox{\textwidth}{!}{\input{thesis/figures/facebook_2010_histogram.tex}}
    \caption{2010}
    \end{subfigure}
    \caption{Facebook MapReduce workloads.}
    \label{fig:facebook:histogram}
\end{figure}

\begin{figure}
    \begin{subfigure}[b]{.33\linewidth}
    \resizebox{\textwidth}{!}{\input{thesis/figures/facebook_2009_0_schedule.tex}}
    \caption{2009-0}
    \end{subfigure}
    \begin{subfigure}[b]{.3075\linewidth}
    \resizebox{\textwidth}{!}{\input{thesis/figures/facebook_2009_1_schedule}}
    \caption{2009-1}
    \end{subfigure}
    \begin{subfigure}[b]{.3475\linewidth}
    \resizebox{\textwidth}{!}{\input{thesis/figures/facebook_2010_schedule}}
    \caption{2010}
    \end{subfigure}
    \caption{Optimal dynamic and static offline schedules for the last day of the Facebook workloads. The left y axis shows the number of servers of the static and dynamic offline optima at a given time (black). The right y axis shows the number of jobs (i.e., the load) at a given time (red).}
    \label{fig:facebook:schedule}
\end{figure}

\begin{table}
    \centering
    \begin{tabularx}{\textwidth}{>{\bfseries}l|X|X|X}
        characteristic & 2009-0 & 2009-1 & 2010 \\\hline
        duration & 1 day & 1 day & 1 day \\
        number of jobs & 6 thousand & 7 thousand & 24 thousand \\
        median interarrival time & 7 seconds & 7 seconds & 2 seconds \\
        PMR & 3.91 & 2.97 & 2.2 \\
        TPMR & 2.04 & 1.93 & 1.69 \\
        mean peak distance & 95 minutes & 106 minutes & 87 minutes \\
        mean valley length & 44 minutes & 36 minutes & 35 minutes \\
        % diurnal pattern & \emph{trace too short} & \emph{trace too short} & \emph{trace too short} \\
    \caption{Characteristics of Facebook's MapReduce workloads.}
    \end{tabularx}
    \label{tab:facebook}
\end{table}

\paragraph{Los Alamos National Lab HPC Traces~\cite{Amvrosiadis2018_3, Amvrosiadis2018, Amvrosiadis2018_2}} This trace comprises two separate traces from high-performance computing clusters from Los Alamos National Lab (LANL). The traces were published by \citeauthor*{Amvrosiadis2018}~\cite{Amvrosiadis2018} as part of the Atlas project at Carnegie Mellon University.

The first trace is from the Mustang cluster, a general-purpose cluster consisting of 1600 homogeneous servers. Jobs were assigned to entire servers. The dataset covers 61 months from October 2011 to November 2016 and is shown in \cref{fig:los_alamos:histogram}. Note that the PMR is large at $622$ due to some outliers in the data. The median job duration is roughly 7 minutes, although the trace includes some extremely long-running outliers, resulting in a mean job duration of over 2.5 hours. In the trace, jobs were assigned to one or multiple servers. To normalize the trace, we consider each job once for each server it was processed on. \Cref{fig:los_alamos:schedule} shows the dynamic and static offline optimal schedules under our second model.

The second trace is from the Trinity supercomputer. This trace is very similar to the Mustang trace but includes an even more significant number of long-running jobs. We, therefore, do not consider this trace in our analysis.

\paragraph{Microsoft Fiddle Trace~\cite{Jeon2019}} This trace consists of deep neural network training workloads on internal servers from Microsoft. The trace was published as part of the Fiddle project from Microsoft Research. The jobs are run on a heterogeneous set of servers which we group based on the number of GPUs of each server. There are 321 servers with two GPUs and 231 servers with eight GPUs. The median job duration is just below 15 minutes. The load profiles are visualized in \cref{fig:microsoft:histogram}.

The CPU utilization of the trace is extremely low, with more than 80\% of servers running with utilization 30\% or less~\cite{Santhanam2019}. However, memory utilization is high, with an average of more than 80\% indicating that overall server utilization is already very high~\cite{Santhanam2019}. Again, the PMR is rather large at 89.43 due to outliers.

In our model, we adjust the runtime of jobs relative to the number of available GPUs in the respective server, i.e., the average runtime of jobs on a 2-GPU-server is four times as long as the average runtime of jobs on an 8-GPU-server. We adjust for the increased energy consumption of a server with eight GPUs by increasing the energy consumption of servers with two GPUs by a factor of 4.2. We also associate a fifteen times higher switching cost with servers with eight GPUs.

The dynamic and static offline optimal schedules under our second model are shown in \cref{fig:microsoft:schedule}. Note that under the given load servers with two GPUs are preferred to servers with eight GPUs when they are only needed for a short period due to their lower switching costs. This might seem counterintuitive at first, as 2-GPU-servers seem to be strictly better than 8-GPU-servers as the operating and switching cost of 8-GPU-servers is worse by a factor greater than four than the respective cost of 2-GPU-servers. However, we assume an average job runtime of 7.5 minutes on 8-GPU-servers as opposed to an average job runtime of 30 minutes on 2-GPU-servers (a factor of four), implying that 8-GPU-servers can process more than four jobs in an hour without a significant increase in delay, whereas 2-GPU-servers are limited to one job per time slot.

\paragraph{Alibaba Trace~\cite{Alibaba2018}} This trace consists of a mixture of long-running applications and batch jobs. We are using their trace from 2018, covering eight days. The trace is visualized in \cref{fig:alibaba:histogram}, the dynamic and static offline optimal schedules under our second model are shown in \cref{fig:alibaba:schedule}. The jobs are processed on 4000 homogeneous servers. In our models, we assume a total of 10,000 servers to ensure that the number of servers is not a bottleneck. Jobs themselves are grouped into 11 types which we further simplify to 4 types based on their average runtime. We consider \emph{short}, \emph{medium}, \emph{long}, and \emph{very long} jobs. Their average runtime in the trace is shown in \cref{tab:alibaba:job_types}. The mean job duration is just below 15 minutes. The median job duration is 8 seconds, and the mean job duration is just over 1.5 minutes.

\begin{table}
    \centering
    \begin{tabularx}{\textwidth}{>{\bfseries}l|c}
        job type & mean runtime \\\hline
        short & 68 seconds \\
        medium & 196 seconds \\
        long & 534 seconds \\
        very long & 1180 seconds \\
    \caption{Characterization of the job types of the Alibaba trace.}
    \end{tabularx}
    \label{tab:alibaba:job_types}
\end{table}

Data from a previous trace indicates that mean CPU utilization varies between 10\% and 40\% while mean memory utilization varies between 40\% and 65\%~\cite{Lu2017}. This indicates that the overall server utilization is not optimal.

In our model, we scale job runtimes by a factor of 2.5 from short to very long jobs, roughly matching the runtimes of jobs from the trace.

\begin{figure}
    \begin{subfigure}[b]{.3425\linewidth}
    \resizebox{\textwidth}{!}{\input{thesis/figures/los_alamos_mustang_histogram.tex}}
    \caption{LANL Mustang}\label{fig:los_alamos:histogram}
    \end{subfigure}
    \begin{subfigure}[b]{.32\linewidth}
    \resizebox{\textwidth}{!}{\input{thesis/figures/microsoft_histogram}}
    \caption{Microsoft Fiddle}\label{fig:microsoft:histogram}
    \end{subfigure}
    \begin{subfigure}[b]{.32\linewidth}
    \resizebox{\textwidth}{!}{\input{thesis/figures/alibaba_histogram}}
    \caption{Alibaba}\label{fig:alibaba:histogram}
    \end{subfigure}
    \caption{LANL Mustang, Microsoft Fiddle, and Alibaba traces. The figures display the average number of job arrivals throughout a day. The interquartile range is shown as the shaded region.}
\end{figure}

\begin{figure}
    \begin{subfigure}[b]{.345\linewidth}
    \resizebox{\textwidth}{!}{\input{thesis/figures/los_alamos_mustang_schedule.tex}}
    \caption{LANL Mustang}\label{fig:los_alamos:schedule}
    \end{subfigure}
    \begin{subfigure}[b]{.305\linewidth}
    \resizebox{\textwidth}{!}{\input{thesis/figures/microsoft_schedule}}
    \caption{Microsoft Fiddle}\label{fig:microsoft:schedule}
    \end{subfigure}
    \begin{subfigure}[b]{.335\linewidth}
    \resizebox{\textwidth}{!}{\input{thesis/figures/alibaba_schedule}}
    \caption{Alibaba}\label{fig:alibaba:schedule}
    \end{subfigure}
    \caption{Optimal dynamic and static offline schedules for the last day of the LANL Mustang, Microsoft Fiddle, and the second to last day of the Alibaba trace. The left y axis shows the number of servers of the static and dynamic offline optima at a given time (black). The right y axis shows the number of jobs (i.e., the load) at a given time (red).}
\end{figure}

\paragraph{} We have seen traces from very different real-world use cases. The Microsoft Fiddle trace is based on a heterogeneous server architecture, and the Alibaba trace receives heterogeneous loads. The PMR and valley lengths of the days used in our analysis are shown in \cref{tab:pmr_vl}. Interestingly, as shown in \cref{tab:traces}, their TPMR, peak distances, and valley lengths are mostly similar.

\subsection{Assumptions}

We impose a couple of assumptions to simplify our analysis. First, and already mentioned, our analysis is inherently limited by the traces we used as a basis for our experiments. While we examine a wide variety of traces, the high variability in traces indicates they are a fundamental limitation to any estimation of real-world performance.

Another common limitation of models is that the interarrival times of jobs are on the order of seconds or smaller~\cite{Amvrosiadis2018}. However, this is not a limitation of our analysis as we are using a general Poisson process with an appropriate mean arrival rate in our delay model.

In the context of high-performance computing, jobs typically have a \emph{gang scheduling}\index{gang scheduling} requirement, i.e., a requirement that related jobs are processed simultaneously even though they are run on different hardware~\cite{Amvrosiadis2018}. For simplification, we assume this requirement always to be satisfied. However, this is not a substantial limitation as the scheduling of jobs within a time slot is not determined by the discussed algorithms and instead left to the server operator. Nevertheless, in principle, the gang scheduling requirement may render some schedules infeasible if the processing time on servers exceeds the length of a time slot when gang scheduling constraints are considered.

There are also some limitations resulting from the design of our model. As was mentioned previously, we assume that the jobs arrive at the beginning of a new time slot rather than at random times throughout the time slot. Moreover, we assumed that for every job, a server type exists that can process this job within one time slot. In other words, there exists no job running longer than $\delta$. We have seen in \cref{section:case_studies:method:traces} that this assumption is violated in most practical scenarios. In \cref{section:application:dynamic_duration}, we described how this assumption can be removed. The same approach can also be used to remove the assumption that jobs must arrive at the beginning of a time slot.

\subsection{Alternatives to Right-Sizing Data Centers}\label{section:case_studies:method:alternatives}

To determine the benefit of dynamically right-sizing data centers, we must first describe the alternative strategies to managing a data center. We will then use these approaches as a point of reference in our analysis.

Most data centers are statically provisioned; that is, the configuration of active servers is only changed rarely (often manually) and remains constant during most periods~\cite{Whitney2014}. To support the highest loads, the data centers are peak-provisioned, i.e., the number of servers is chosen such that they suffice to process all jobs even during times where most jobs arrive. Moreover, as a safety measure, data centers are typically provisioned to handle much higher loads than the loads encountered in practice~\cite{Whitney2014}.

\begin{table}
    \centering
    \begin{tabularx}{\textwidth}{>{\bfseries}l|X|X|X}
        characteristic & LANL Mustang & Microsoft Fiddle & Alibaba \\\hline
        duration & 5 years & 30 days & 8 days \\
        number of jobs & 20 million & 120 thousand & 14 million \\
        median interarrival time & 0 seconds & 8 seconds & 0 seconds \\
        PMR & 621.94 & 89.43 & 3.93 \\
        TPMR & 2.5 & 1.68 & 1.77 \\
        mean peak distance & 100 minutes & 105 minutes & 89 minutes \\
        mean valley length & 120 minutes & 115 minutes & 74 minutes \\
        diurnal pattern & yes & - & yes \\
    \caption{Characteristics of the LANL Mustang, Microsoft Fiddle, and Alibaba traces.}
    \end{tabularx}
    \label{tab:traces}
\end{table}

Naturally, traces with a high PMR or long valleys are more likely to benefit from alternatives to static provisioning. Therefore another widely used alternative is \emph{valley filling}\index{valley filling}, which aims to schedule lower priority jobs (i.e., some batch jobs) during valleys. In an ideal scenario, this approach can achieve $\text{PMR} \approx 1$, which would allow for efficient static provisioning. Crucially, this approach requires a large number of low-priority jobs which may be processed with a significant delay (requiring a considerable minimum perceptible delay $\delta_i$ for a large number of jobs of type $i$), and thus in most cases, valleys cannot be eliminated entirely. \citeauthor*{Lin2011}~\cite{Lin2011} showed that dynamic-right sizing can be combined with valley filling to achieve a significant cost reduction. The optimal balancing of dynamic right-sizing and valley filling is mainly determined by the change to the PMR. \citeauthor*{Lin2011}~\cite{Lin2011} showed that cost savings of 20\% are possible with a PMR of 2 and a PMR of approximately 1.3 can still achieve cost savings of more than 5\%. Generally, the cost reduction vanishes once the PMR approaches $1$, which may happen between 30\% to 70\% mean background load~\cite{Lin2011}. The results when dynamic right-sizing is used together with valley filling can be estimated from previous results.

\subsection{Performance Metrics}

Let $OPT$ denote the dynamic offline optimum and $OPT_s$ denote the static offline optimum. In our analysis, the \emph{normalized cost}\index{normalized cost} of an online algorithm is the ratio of the obtained cost and the dynamic optimal offline cost, i.e. $NC(ALG) = c(ALG) / c(OPT)$. Further, we base our estimated \emph{cost reduction}\index{cost reduction} on an optimal offline static provisioning: \begin{align*}
    CR(ALG) = \frac{c(OPT_s) - c(ALG)}{c(OPT_s)}.
\end{align*} Note that this definition is similar to the definition of regret, but expressed relative to the overall cost. We refer to $SDR = c(OPT_s) / c(OPT)$ as the \emph{static/dynamic ratio}\index{static/dynamic ratio}, which is closely related to the \emph{potential cost reduction}\index{potential cost reduction} $PCR = CR(OPT)$.

\subsection{Previous Results}

\citeauthor*{Lin2011}~\cite{Lin2011} showed that the cost reduction is directly proportional to the PMR and inversely proportional to the normalized switching cost. Additionally, \citeauthor*{Lin2011}~\cite{Lin2011} showed that, as one would expect, the possible cost reduction decreases as the delay cost assumes a more significant fraction of the overall hitting costs. In practice, this can be understood as the effect of making the model more conservative.

\begin{table}
    \centering
    \begin{tabularx}{\textwidth}{>{\bfseries}l|c|c}
        trace & PMR & mean valley length (hours) \\\hline
        Facebook 2009-0 & 2.115 & 2.565 \\
        Facebook 2009-1 & 1.913 & 1.522 \\
        Facebook 2010 & 1.549 & 1.435 \\
        LANL Mustang & 6.575 & 1.167 \\
        Microsoft Fiddle & 3.822 & 2.125 \\
        Alibaba & 1.339 & 2.792 \\
    \caption{PMR and mean valley length of the traces used in our analysis. Note that the valley lengths are typically shorter than the normalized switching cost of our model.}
    \end{tabularx}
    \label{tab:pmr_vl}
\end{table}

\subsection{Model Parameters}\label{section:case_studies:traces:model-parameters}

We now describe how we parametrized our model in our case studies. In our models, we strive to choose conservative estimates to under-estimate the cost savings from dynamically right-sizing data centers. This approach is similar to the study by \citeauthor*{Lin2011}~\cite{Lin2011}. \Cref{tab:model} gives an overview of the used parameters producing the results of subsequent sections.

\paragraph{Energy} We use the linear energy consumption model from \autoref{eq:energy_model:1} in our experiments. In their analysis, \citeauthor*{Lin2011}~\cite{Lin2011} choose energy cost and energy consumption such that the fixed energy cost (i.e., the energy cost of a server when idling) is $1$ and the dynamic energy cost is $0$ as, on most servers, the fixed costs dominate the dynamic costs~\cite{Clark2005}. We investigate this model and an alternative model. In the alternative model, we estimate the power consumption of a server with 1 kW during peak loads and with 500 W when idling to yield a conservative estimate (as cooling costs are included). According to the U.S. Energy Information Administration (EIA), the average cost of energy in the industrial sector in the United States during April 2021 was 6.77 cents per kilowatt-hour~\cite{EIA2021}. We use this as a conservative estimate as data centers typically use a more expensive portfolio of energy sources. If the actual carbon cost of the used energy were to be considered, which is the case in some data centers as discussed in \cref{section:application:operating_cost:energy}, energy costs are likely to be substantially higher.

\paragraph{Revenue Loss} According to measurements, a 500 ms increase in delay results in a revenue loss of 20\% or 0.04\%/ms~\cite{Lin2012, Hamilton2009}. Thus, scaling the delay measured in ms by 0.1 can be used as a slight over-approximation of revenue loss. \citeauthor*{Lin2011}~\cite{Lin2011} choose the minimal perceptible delay as 1.5 times the time to run a job, which is a very conservative estimate if valley filling is assumed a viable alternative. In our model, we choose the minimal perceptible delay as 2.5 times the time to run a job which is equivalent as we also added the processing time of a job to the delay. In the case of valley filling, jobs are typically processed with a much more significant delay. Similar to \citeauthor*{Lin2012}~\cite{Lin2012}, we also estimate a constant network delay of 10 ms.

\paragraph{Switching Cost} We mentioned in \cref{section:application:switching_cost} that in practice, the switching cost should be on the order of operating a server between an hour to several hours. To obtain a conservative estimate, we choose $\beta$ such that the normalized switching cost times the length of a time slot equals 4 hours.

\paragraph{Time Slot Length} We choose a time slot length of 1 hour. We further assume that the average processing time of jobs is $\delta / 2$ unless noted otherwise.

\begin{table}
    \centering
    \begin{tabularx}{\textwidth}{>{\bfseries}l|X|X}
        parameter & model 1 & model 2 \\\hline
        time slot length & 1 hour & 1 hour \\
        energy cost & $c=1$ & $c=0.0677$ \\
        energy consumption & $\Phi_{\text{min}}=1, \Phi_{\text{max}}=1$ & $\Phi_{\text{min}}=0.5, \Phi_{\text{max}}=1$ \\
        revenue loss & $\gamma = 0.1, \delta_i = 2.5 \eta_i$ & $\gamma = 0.1, \delta_i = 2.5 \eta_i$ \\
        normalized switching cost & 4 hours & 4 hours \\
    \caption{Models used in our case studies. $\eta_i$ is the processing time of jobs of type $i$.}
    \end{tabularx}
    \label{tab:model}
\end{table}

\section{Uni-Dimensional Algorithms}

The results of this section are based on the final day of the LANL Mustang, Facebook, and the second to last day of the Alibaba trace. We begin by discussing the general features of the traces. Then, we compare the uni-dimensional online algorithms with respect to their achieved normalized cost, cost reduction, and runtime.

\paragraph{Fractional vs. Integral Cost} For all traces, the ratio of the fractional and the integral costs is 1 for a precision of at least $10^{-3}$. This is not surprising due to the large number of servers used in each model.

\begin{figure}
    \begin{subfigure}[b]{.5\linewidth}
    \resizebox{\textwidth}{!}{\input{thesis/figures/opt_vs_opts}}
    \caption{Ratio of static and dynamic optima}
    \end{subfigure}
    \begin{subfigure}[b]{.5\linewidth}
    \resizebox{\textwidth}{!}{\input{thesis/figures/opts_opt_vs_pmr}}
    \caption{Correlation of the PMR and the static/dynamic ratio}\label{fig:case_studies:ud:opt_vs_opts:pmr}
    \end{subfigure}
    \caption{Ratio of static and dynamic offline optima for each trace. The LANL Mustang and Microsoft Fiddle traces have a significantly higher PMR than the remaining traces. Generally, we observe a strong correlation of PMR and the static/dynamic ratio.}\label{fig:case_studies:ud:opt_vs_opts}
\end{figure}

\begin{figure}
    \begin{subfigure}[b]{.49\linewidth}
    \resizebox{\textwidth}{!}{\input{thesis/figures/opt_vs_opts_against_normalized_cost}}
    \caption{Normalized cost}\label{fig:case_studies:ud:opt_vs_opts_against_normalized_cost}
    \end{subfigure}
    \begin{subfigure}[b]{.51\linewidth}
    \resizebox{\textwidth}{!}{\input{thesis/figures/opt_vs_opts_against_mean_cost_reduction}}
    \caption{Cost reduction}\label{fig:case_studies:ud:opt_vs_opts_against_mean_cost_reduction}
    \end{subfigure}
    \caption{Effect of the ratio of static and dynamic optima on the cost reduction and normalized cost achieved by the memoryless algorithm.}
\end{figure}

\paragraph{Dynamic vs. Static Cost} The dynamic and static costs differ significantly depending on the trace. The ratio of dynamic and static optimal costs for each trace is shown in \cref{fig:case_studies:ud:opt_vs_opts}.

\Cref{fig:case_studies:ud:opt_vs_opts_against_mean_cost_reduction} shows a strong positive correlation between the average cost reduction achieved by the memoryless algorithm and the ratio of the static and dynamic optima. As $OPT_s / OPT$ is directly linked to the PMR, this also indicates a strong correlation between cost reduction and the PMR. Even under our very conservative estimates of parameters, we achieve a significant cost reduction when the ratio of the static and dynamic offline optimum exceeds 1.5. Similar to \citeauthor*{Lin2011}~\cite{Lin2011}, we observe that cost savings increase rapidly as the PMR increases.

We also observe in \cref{fig:case_studies:ud:opt_vs_opts_against_normalized_cost} that as the static/dynamic ratio increases, the normalized costs achieved by the memoryless algorithm increases too but not as much as the potential energy savings, resulting in the observed significant cost reduction.

\begin{figure}
    \begin{subfigure}[b]{.3425\linewidth}
    \resizebox{\textwidth}{!}{\input{thesis/figures/fb90_normalized_cost}}
    \caption{Facebook 2009-0}
    \end{subfigure}
    \begin{subfigure}[b]{.32\linewidth}
    \resizebox{\textwidth}{!}{\input{thesis/figures/fb91_normalized_cost}}
    \caption{Facebook 2009-1}
    \end{subfigure}
    \begin{subfigure}[b]{.32\linewidth}
    \resizebox{\textwidth}{!}{\input{thesis/figures/fb10_normalized_cost}}
    \caption{Facebook 2010}
    \end{subfigure}
    \par\bigskip
    \begin{subfigure}[b]{.50\linewidth}
    \resizebox{\textwidth}{!}{\input{thesis/figures/lanl_normalized_cost}}
    \caption{LANL Mustang}
    \end{subfigure}
    \begin{subfigure}[b]{.48\linewidth}
    \resizebox{\textwidth}{!}{\input{thesis/figures/alibaba_normalized_cost}}
    \caption{Alibaba}
    \end{subfigure}
    \caption{Normalized costs of uni-dimensional online algorithms. For the Facebook 2010 trace, the second model results in an optimal schedule constantly using all servers, explaining the disparate performance compared to the first model. Further, Probabilistic and Rand-Probabilistic perform very poorly in this setting and are therefore not shown for model 2. Generally, Probabilistic and Rand-Probabilistic achieve similar results. Interestingly, we observe that LCP outperforms Int-LCP when the potential cost reduction is small. In contrast, Int-LCP and the probabilistic algorithms outperform Memoryless and LCP significantly when the potential cost reduction is large. \Cref{fig:case_studies:ud:lcp_vs_int_lcp} compares the schedules obtained by LCP and Int-LCP in greater detail.}\label{fig:case_studies:ud:normalized_cost}
\end{figure}

\begin{figure}
    \begin{subfigure}[b]{.3425\linewidth}
    \resizebox{\textwidth}{!}{\input{thesis/figures/fb90_cost_reduction}}
    \caption{Facebook 2009-0}
    \end{subfigure}
    \begin{subfigure}[b]{.32\linewidth}
    \resizebox{\textwidth}{!}{\input{thesis/figures/fb91_cost_reduction}}
    \caption{Facebook 2009-1}
    \end{subfigure}
    \begin{subfigure}[b]{.32\linewidth}
    \resizebox{\textwidth}{!}{\input{thesis/figures/fb10_cost_reduction}}
    \caption{Facebook 2010}
    \end{subfigure}
    \par\bigskip
    \begin{subfigure}[b]{.50\linewidth}
    \resizebox{\textwidth}{!}{\input{thesis/figures/lanl_cost_reduction}}
    \caption{LANL Mustang}
    \end{subfigure}
    \begin{subfigure}[b]{.48\linewidth}
    \resizebox{\textwidth}{!}{\input{thesis/figures/alibaba_cost_reduction}}
    \caption{Alibaba}
    \end{subfigure}
    \caption{Cost reduction of uni-dimensional online algorithms. For the Facebook 2010 trace, the second model results in an optimal schedule constantly using all servers, explaining the disparate performance compared to the first model. Further, Probabilistic and Rand-Probabilistic perform very poorly in this setting and are therefore not shown for model 2. Results are mainly determined by the normalized cost and the potential cost reduction (or static/dynamic ratio). We observe that the achieved cost reduction is dominated by the potential cost reduction (see \cref{fig:case_studies:ud:cost_reduction_vs_normalized_cost}).}\label{fig:case_studies:ud:cost_reduction}
\end{figure}

\begin{figure}
    \centering
    \input{thesis/figures/cr_vs_nc}
    \caption{Weak positive correlation of achieved normalized cost and cost reduction. Intuitively, one would expect a strong negative correlation, i.e., the achieved cost reduction increases as the normalized cost approaches 1. Here, we find a positive correlation as the achieved cost reduction is dominated by the potential cost reduction (see \cref{fig:case_studies:ud:opt_vs_opts_against_mean_cost_reduction}).}\label{fig:case_studies:ud:cost_reduction_vs_normalized_cost}
\end{figure}

\paragraph{Normalized Cost} In the application of right-sizing data centers, we are interested in the cost associated with integral schedules. \Cref{fig:case_studies:ud:normalized_cost} shows the normalized cost of each algorithm. For fractional algorithms, we consider the cost of the associated integral schedule obtained by ceiling each configuration. Notably, LCP and Int-LCP perform differently depending on the trace and used model. We explore this behavior in \cref{fig:case_studies:ud:lcp_vs_int_lcp}.

\paragraph{Cost Reduction} \Cref{fig:case_studies:ud:cost_reduction} shows the achieved cost reduction. In general, we observe in \cref{fig:case_studies:ud:opt_vs_opts_against_normalized_cost}, \cref{fig:case_studies:ud:opt_vs_opts_against_mean_cost_reduction}, and \cref{fig:case_studies:ud:cost_reduction_vs_normalized_cost} that the achieved cost reduction is dominated by the potential cost reduction (which is mainly influenced by the PMR, see \cref{fig:case_studies:ud:opt_vs_opts:pmr}). When the potential cost reduction is small, algorithms with a smaller normalized cost in a particular setting, achieve a significantly higher cost reduction.

\begin{figure}
    \begin{subfigure}[b]{.5175\linewidth}
    \resizebox{\textwidth}{!}{\input{thesis/figures/lcp_vs_ilcp_1}}
    \caption{Facebook 2009-0}
    \end{subfigure}
    \begin{subfigure}[b]{.4825\linewidth}
    \resizebox{\textwidth}{!}{\input{thesis/figures/lcp_vs_ilcp_2}}
    \caption{LANL Mustang}
    \end{subfigure}
    \caption{Comparison of the schedules obtained by LCP and Int-LCP under our first model. We observe that LCP is much ``stickier'' than Int-LCP, which is beneficial when the potential cost reduction is small (i.e., for the Facebook 2009-0 trace) but detrimental when the potential cost reduction is large (i.e., for the LANL Mustang trace). In our experiments, we observe that the memoryless algorithm tends to behave similarly to LCP (i.e., is more ``sticky''), whereas the probabilistic algorithms tend to behave similarly to Int-LCP (i.e., are less ``sticky'').}\label{fig:case_studies:ud:lcp_vs_int_lcp}
\end{figure}

\begin{figure}
    \begin{subfigure}[b]{.5175\linewidth}
    \resizebox{\textwidth}{!}{\input{thesis/figures/ud_runtimes}}
    \end{subfigure}
    \begin{subfigure}[b]{.4825\linewidth}
    \resizebox{\textwidth}{!}{\input{thesis/figures/ud_runtimes_slow}}
    \end{subfigure}
    \caption{Runtimes of uni-dimensional online algorithms.}\label{fig:case_studies:ud:runtimes}
\end{figure}

\paragraph{Runtime} \cref{fig:case_studies:ud:runtimes} shows the distribution of runtimes (per iteration) of the online algorithms using the Facebook 2009-1 trace. The memoryless algorithm, LCP, and Int-LCP are very fast, even as the number of time slots increases. The runtime of Probabilistic and Rand-Probabilistic is slightly dependent on the used trace and model but generally good. However, when resulting schedules are thight around the upper bound of the decision space, as is the case for the Facebook 2010 trace under our second model, the probabilistic algorithms perform take multiple minutes per iteration. Rand-Probabilistic is significantly slower than Probabilistic as due to the relaxation the integrals need to be computed in a piecewise fashion. The runtime of RBG grows linearly with time and is shown in \cref{fig:case_studies:ud:runtimes} for the first four time slots.

\begin{figure}
    \centering
    \input{thesis/figures/costs}
    \caption{Cost profiles of uni-dimensional online algorithms. Note that integral algorithms seem to prefer revenue loss and switching costs over energy costs, whereas fractional algorithms prefer energy costs. However, this is likely because integral algorithms balance energy costs and revenue loss more accurately than the ceiled schedules of fractional algorithms.}\label{fig:case_studies:ud:costs}
\end{figure}

\paragraph{Cost Makeup} An interesting aspect of the (integral) schedules obtained by the online algorithms is the makeup of their associated costs to understand whether an algorithm systematically prefers some cost over another. We measure this preference of an algorithm as the normalized deviation, i.e., the cost of the algorithm minus the mean cost among all algorithms divided by the standard deviation. We then average the results between all traces. \Cref{fig:case_studies:ud:costs} shows the cost profiles for each algorithm. We observe that fractional algorithms prefer energy cost over revenue loss and switching cost, while integral algorithms prefer revenue loss and switching cost over energy cost. This is likely because fractional algorithms cannot balance energy cost and revenue loss optimally. When fractional schedules are ceiled, this results in an additional energy cost while reducing revenue loss. In absolute terms, the deviations make up less than 1\% of the overall costs of each type.

\paragraph{} We have seen that even in our conservative model, significant cost savings with respect to the optimal static provisioning in hindsight can be achieved in practical settings when the PMR is large enough or the normalized switching cost is less than the typical valley length. Due to the conservative estimates of our model, it is likely that in practice, much more drastic cost savings are possible. For example, when energy costs are higher, or the switching costs are on the order of operating a server in an idle state for one hour rather than four hours.

\section{Multi-Dimensional Algorithms}

Now, we turn to the discussed multi-dimensional algorithms. We begin by analyzing the simplified settings, SLO and SBLO, from \cref{section:online_algorithms:md:lazy_budgeting}. Then, we analyze the gradient-based methods from \cref{section:online_algorithms:md:descent_methods}.

\subsection{Smoothed Load Optimization}

Recall that for SLO, during a single time slot a server can process at most one job. Hence, we cannot use dynamic job durations to model the different runtimes of jobs on servers with two GPUs and servers with eight GPUs. Instead, we use a simplified model based on our second model, which is described in \cref{tab:simp_model}. Note that we disregard revenue loss and that we assume, servers operate at full utilization (if they are active). Overall, we obtain the operating costs $c = (243.720, 219.348)$ and switching costs $\beta = (487.440, 663.672)$.

\begin{table}
    \centering
    \begin{tabularx}{\textwidth}{>{\bfseries}l|X}
        cost & simplified model \\\hline
        operating cost & servers with eight GPUs have $0.9$ times the energy consumption (per processed job) as servers with two GPUs due to improved cooling efficiency \\
        switching cost & servers with eight GPUs have $1.3$ times the switching cost as servers with two GPUs due to an increased associated risk \\
    \caption{Simplified model used in our case studies of SLO and SBLO. The model parameters are based on our second model described in \cref{tab:model}.}
    \end{tabularx}
    \label{tab:simp_model}
\end{table}

The achieved normalized cost and cost reduction of lazy budgeting are shown in \cref{fig:case_studies:md:slo:normalized_cost} and \cref{fig:case_studies:md:slo:cost_reduction}, respectively. The dynamic offline optimal schedule primarily uses 8-GPU-servers and only uses 2-GPU-servers for short periods. The static offline optimal schedule uses 122 8-GPU-servers and no 2-GPU-servers as they have a larger operating cost, which would have to be paid throughout the entire day. The lazy budgeting algorithms primarily use 2-GPU-servers due to their lower switching cost and stick with 8-GPU-servers once they were powered up. \Cref{fig:case_studies:md:slo:det:schedule} and \cref{fig:case_studies:md:slo:rand:schedule} show the schedules obtained by the online algorithms in comparison to the offline optimal schedule. The runtime of the deterministic and randomized variants is shown in \cref{fig:case_studies:md:slo:runtimes}.

\begin{figure}
    \begin{subfigure}[b]{.5\linewidth}
    \resizebox{\textwidth}{!}{\input{thesis/figures/slo_normalized_cost}}
    \caption{Normalized cost}\label{fig:case_studies:md:slo:normalized_cost}
    \end{subfigure}
    \begin{subfigure}[b]{.5\linewidth}
    \resizebox{\textwidth}{!}{\input{thesis/figures/slo_cost_reduction}}
    \caption{Cost reduction}\label{fig:case_studies:md:slo:cost_reduction}
    \end{subfigure}
    \caption{Performance of lazy budgeting (SLO) for the Microsoft Fiddle trace when compared against the offline optimum. The results of the randomized algorithm are based on five individual runs.}
\end{figure}

\begin{figure}
    \begin{subfigure}[b]{.3425\linewidth}
    \resizebox{\textwidth}{!}{\input{thesis/figures/slo_det_schedule}}
    \caption{SLO (deterministic)}\label{fig:case_studies:md:slo:det:schedule}
    \end{subfigure}
    \begin{subfigure}[b]{.32\linewidth}
    \resizebox{\textwidth}{!}{\input{thesis/figures/slo_rand_schedule}}
    \caption{SLO (randomized)}\label{fig:case_studies:md:slo:rand:schedule}
    \end{subfigure}
    \begin{subfigure}[b]{.32\linewidth}
    \resizebox{\textwidth}{!}{\input{thesis/figures/sblo_schedule}}
    \caption{SBLO ($\epsilon = 1/4$)}\label{fig:case_studies:md:sblo:schedule}
    \end{subfigure}
    \caption{Comparison of the schedules obtained by lazy budgeting for the Microsoft Fiddle trace and the offline optimum. For SLO, the deterministic algorithm is shown in blue and one result of the randomized algorithm is shown in red. The lazy budgeting algorithm for SBLO is shown in green. Note that the lazy budgeting algorithms prefer the 2-GPU-servers initially due to their low switching costs. For SLO, the randomized algorithm appears to be less ``sticky'' than the deterministic algorithm, resulting in a better normalized cost.}
\end{figure}

\begin{figure}
    \begin{subfigure}[b]{.5175\linewidth}
    \resizebox{\textwidth}{!}{\input{thesis/figures/slo_runtimes}}
    \caption{SLO}\label{fig:case_studies:md:slo:runtimes}
    \end{subfigure}
    \begin{subfigure}[b]{.4825\linewidth}
    \resizebox{\textwidth}{!}{\input{thesis/figures/sblo_runtimes}}
    \caption{SBLO}\label{fig:case_studies:md:sblo:runtimes}
    \end{subfigure}
    \caption{Runtime of lazy budgeting algorithms.}
\end{figure}

\subsection{Smoothed Balanced-Load Optimization}

For our analysis of SBLO, we use the same simplified model that we used in our analysis of SLO (see \cref{tab:simp_model}). In particular, we still assume that a server can at most process a single job during a time slot. The dynamic and static offline optimum are similar to those in our analysis of SLO. In particular, the static offline optimum still only uses 8-GPU-servers. \Cref{fig:case_studies:md:sblo:schedule} shows the schedule obtained by lazy budgeting ($\epsilon = 1/4$) in comparison with the offline optimal. Lazy budgeting achieves normalized costs 1.284 and a cost reduction of 11\%. The runtime of the algorithm is shown in \cref{fig:case_studies:md:sblo:runtimes}.

\subsection{Descent Methods}

We also evaluated the performance of P-OBD and D-OBD on the Microsoft Fiddle trace under our original models (see \cref{tab:model}). In our analysis we use the squared $\ell_2$ norm as the distance-generating function, i.e. $h(x) = \frac{1}{2} \norm{x}_2^2$, which is strongly convex and Lipschitz smooth in the $\ell_2$ norm. In our data center model, we use the $\ell_1$ norm to calculate switching costs, however, we observe that this approximation still achieves a good performance when compared against the dynamic offline optimum. The negative entropy $h(x) = \sum_{k=1}^d x_k \log_2 x_k$, which is commonly used as a distance-generating function for the $\ell_1$ norm cannot be used in the right-sizing data center setting as $\mathbf{0} \in \mathcal{X}$. \Cref{fig:case_studies:md:obd:normalized_cost} and \cref{fig:case_studies:md:obd:cost_reduction} show the achieved normalized cost and cost reduction. The resulting schedules under the first model are compared with the offline optimal in \cref{fig:case_studies:md:obd:schedule}. Remarkably, P-OBD and D-OBD obtain the exact same schedule under our second model. \Cref{fig:case_studies:md:obd:runtimes} visualizes the runtime of P-OBD and D-OBD under our first model.

\begin{figure}
    \begin{subfigure}[b]{.5\linewidth}
    \resizebox{\textwidth}{!}{\input{thesis/figures/obd_normalized_cost}}
    \caption{Normalized cost}\label{fig:case_studies:md:obd:normalized_cost}
    \end{subfigure}
    \begin{subfigure}[b]{.5\linewidth}
    \resizebox{\textwidth}{!}{\input{thesis/figures/obd_cost_reduction}}
    \caption{Cost reduction}\label{fig:case_studies:md:obd:cost_reduction}
    \end{subfigure}
    \caption{Performance of P-OBD  ($\beta = 1/2$) and D-OBD  ($\eta = 1$) for the Microsoft Fiddle trace when compared against the offline optimum. $h(x) = \frac{1}{2} \norm{x}_2^2$ is used as the distance-generating function.}
\end{figure}

\begin{figure}
    \begin{subfigure}[b]{.5175\linewidth}
    \resizebox{\textwidth}{!}{\input{thesis/figures/pobd_schedule}}
    \caption{P-OBD}
    \end{subfigure}
    \begin{subfigure}[b]{.4825\linewidth}
    \resizebox{\textwidth}{!}{\input{thesis/figures/dobd_schedule}}
    \caption{D-OBD}
    \end{subfigure}
    \caption{Comparison of the schedules obtained by OBD for the Microsoft Fiddle trace under our first model and the offline optimum. P-OBD ($\beta = 1/2$) is shown in blue and D-OBD ($\eta = 1$) is shown in red. The two time slots during which P-OBD and D-OBD differ are marked in yellow ($t \in \{6, 20\}$). In both time slots, D-OBD is slightly less ``sticky'', resulting in a slightly better normalized cost. Also observe that under our first model, the dynamic offline optimum strictly prefers 2-GPU-servers over 8-GPU-servers In contrast, under our second model, 8-GPU-servers are slightly preferred by the dynamic offline optimum (see \cref{fig:microsoft:schedule}). $h(x) = \frac{1}{2} \norm{x}_2^2$ is used as the distance-generating function.}\label{fig:case_studies:md:obd:schedule}
\end{figure}

Although, OBD incurs an increased cost compared to the static offline optimal (the optimal static choice in hindsight) albeit by a factor less than 1, our very conservative model indicates that in practice, significant cost savings are possible.

\begin{figure}
    \centering
    \input{thesis/figures/obd_runtimes}
    \caption{Runtime of OBD algorithms.}\label{fig:case_studies:md:obd:runtimes}
\end{figure}

\section{Predictions}

In our analysis, we use the Alibaba trace under our second model to evaluate the effects of using predictions. We consider two different types of predictions: perfect predictions, and actual predictions that are obtained as described in \cref{section:online_algorithms:md:predictions:making_predictions}. The obtained predictions are based on the four preceding days of the Alibaba trace up until the second to last day. \Cref{fig:case_studies:predictions:prediction} visualizes the used prediction for the most common job type (i.e., short jobs). Note that in our analysis, we use the mean predicted load.

\begin{figure}
    \centering
    \input{thesis/figures/prediction}
    \caption{Prediction of the load of short jobs for the second to last day of the Alibaba trace. The mean prediction is shown as the black line. The interquartile range of the predicted distribution is shown as the shaded region. The marks represent actual loads.}\label{fig:case_studies:predictions:prediction}
\end{figure}

\Cref{fig:case_studies:predictions:lcp} shows the effect of the prediction window $w$ when used with LCP for perfect and actual predictions. We observe that in practice, the prediction window can significantly improve the algorithm performance. Additionally, we find that this effect is also achieved with imperfect (i.e., actual) predictions. Previously, \citeauthor*{Lin2011}~\cite{Lin2011} only showed this effect for perfect predictions with additive white Gaussian noise.

Interestingly, RHC and AFHC achieve equivalent results for perfect and imperfect predictions. \Cref{fig:case_studies:predictions:mpc} shows how the achieved normalized cost changes with the prediction window. Crucially, note that the MCP-style algorithms do not necessarily perform better for a growing prediction window. \citeauthor*{Lin2012}~\cite{Lin2012} showed this effect previously for an adversarially chosen example, however, we observe this behavior with AFHC in a practical setting. In fact, for the Alibaba trace, RHC and AFHC achieve their best result when used without a prediction window, i.e. $w = 0$.

\begin{figure}
    \begin{subfigure}[b]{.5175\linewidth}
    \resizebox{\textwidth}{!}{\input{thesis/figures/pred_lcp_cost}}
    \caption{LCP and Int-LCP}\label{fig:case_studies:predictions:lcp}
    \end{subfigure}
    \begin{subfigure}[b]{.4825\linewidth}
    \resizebox{\textwidth}{!}{\input{thesis/figures/pred_mpc_cost}}
    \caption{RHC and AFHC}\label{fig:case_studies:predictions:mpc}
    \end{subfigure}
    \caption{Performance of online algorithms with a prediction window for the Alibaba trace. The left figure shows the performance of LCP and Int-LCP. The right figure shows the performance of RHC and AFHC. Note that LCP does not continuously approach the normalized cost $1$ as $w \to 24$ because of numerical inaccuracies solving the convex optimizations and as the obtained is compared to the integral offline optimum rather than the fractional offline optimum. For RHC and AFHC, the achieved normalized cost is independent of whether perfect predictions or the predictions from \cref{fig:case_studies:predictions:prediction} are used.}
\end{figure}
